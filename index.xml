<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Chili Pepper</title>
    <link>https://hpc.stat.yonsei.ac.kr/</link>
    <description>Recent content on Chili Pepper</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 15 Mar 2022 14:54:35 +0900</lastBuildDate><atom:link href="https://hpc.stat.yonsei.ac.kr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Use GPU Node for SLURM</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/how-to-use-gpu-node-for-slurm/</link>
      <pubDate>Tue, 15 Mar 2022 14:54:35 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/how-to-use-gpu-node-for-slurm/</guid>
      <description>How to use GPU node for SLURM Step 1. Export your conda setting conda 환경을 동일하게 맞춰주기 위해 local에서 생성된 가상환경으로부터 환경설정 파일을 만들고, 서버에서 conda 가상환경을 만들 때 사용한다.
1 2 3 4 5 6 7 8 9 10 11  # export conda setting conda activate [YOUR ENV NAME] conda env export -n [ENV NAME] -f [FILENAME].yml --no-builds # 이러면 조금은 해결되긴 함 # create environment from file conda create --name [YOUR ENV NAME] python = [VESTION] # same env name in yml file conda env create -f [FILENAME].</description>
    </item>
    
    <item>
      <title>SLURM Documentation</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/</link>
      <pubDate>Tue, 08 Mar 2022 22:39:38 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/</guid>
      <description>1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.
If you need more information, Please Visit https://slurm.schedmd.com/overview.html
2. Basic SLURM Command   sbatch
 Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output &amp;gt; Submitted batch job 210 # job_id: 210     squeue</description>
    </item>
    
    <item>
      <title>How do I use the Chili Pepper Cluster?</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/using-slurm/</link>
      <pubDate>Mon, 21 Feb 2022 12:12:53 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/using-slurm/</guid>
      <description>Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q&amp;amp;A channel in the Slack Group.
Step 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/.</description>
    </item>
    
    <item>
      <title>Creating Your Custom Jupyter Kernel from a Virtual Environment</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/</link>
      <pubDate>Sun, 28 Nov 2021 22:24:04 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/</guid>
      <description>Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.
1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way.</description>
    </item>
    
  </channel>
</rss>
