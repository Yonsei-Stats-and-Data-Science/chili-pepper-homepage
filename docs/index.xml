<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docs on Chili Pepper</title>
    <link>https://hpc.stat.yonsei.ac.kr/docs/</link>
    <description>Recent content in Docs on Chili Pepper</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 04 Oct 2022 14:54:35 +0900</lastBuildDate><atom:link href="https://hpc.stat.yonsei.ac.kr/docs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2. CPU node 사용법(Python)</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/02_how-to-use-cpu-node_python/</link>
      <pubDate>Tue, 04 Oct 2022 14:54:35 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/02_how-to-use-cpu-node_python/</guid>
      <description>CPU node에서 Python 코드 실행하기 R 사용자는 Step 1-3을 숙지한 뒤 다음 문서로 넘어가세요.
Step 1 - terminal 앱 고르기 User는 SSH로 proxy node에 접속하여 클러스터를 사용합니다. 터미널 환경과 vi 에디터에 익숙한 user는 자신에게 친숙한 앱을 사용하면 됩니다. 그렇지 않은 경우 Visual Studio Code를 사용하는 것을 추천합니다. 이 문서에서는 Visual Studio Code를 사용하는 것을 전제로 합니다. 추천 이유는 다음과 같습니다.
 Windows, MacOS, Linux에서 모두 사용 가능합니다. 터미널과 에디터, 파일 브라우저가 통합되어 있습니다.</description>
    </item>
    
    <item>
      <title>5. 모바일 기기에서(iOS/Android) 클러스터 사용하기</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/05_ipad/</link>
      <pubDate>Sat, 19 Mar 2022 14:15:35 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/05_ipad/</guid>
      <description>1. iOS에서 클러스터 사용하기 2번 문서에서 설명했듯이, 어느 환경에서든 SSH 접속만 가능하다면 클러스터 이용이 가능합니다. Visual Studio Code를 쓸 수 있다면 좋겠지만, iOS 버전이 없습니다. Web version은 SSH를 지원하지 않습니다. 이 문서는 iPad에서 koder라는 앱을 이용해 클러스터를 이용하는 예제를 다룹니다.
1) 파일 시스템 접속 클러스터의 파일 시스템에 접속하여 파일을 열람하고 수정할 수 있습니다.   좌측 상단의 2번째 아이콘을 클릭하여 FTP/SFTP 설정으로 들어갑니다.
  좌측 하단의 + 버튼을 누르면 New FTP Connection 설정 창이 나옵니다.</description>
    </item>
    
    <item>
      <title>3. CPU node 사용법(R)</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/03_how-to-use-cpu-node_r/</link>
      <pubDate>Sat, 12 Mar 2022 13:54:35 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/03_how-to-use-cpu-node_r/</guid>
      <description>3. CPU node에서 R 코드 실행하기 2번 문서의 Step 1, 2, 3을 먼저 숙지하시기 바랍니다. 이 문서는 그 이후의 내용만을 다룹니다.
1. 필요한 R 패키지를 자신의 디렉토리에 설치하기 R은 cpu-compute에만 설치되어 있습니다. R은 conda environment를 사용하지 않으며, R 패키지들은 install.packages를 통해 설치할 때 별도의 옵션을 주지 않으면 user별 directory가 아닌 NAS 내의 공통 폴더에 저장됩니다.
아래의 샘플 코드는 R 패키지를 /mnt/nas/users/mjm/R_packages 라는 디렉토리에 설치합니다. **install.packages()**에서
 lib 옵션을 통해 자신의 개인 디렉토리에 패키지를 설치할 수 있습니다.</description>
    </item>
    
    <item>
      <title>4. GPU node 사용법(Python)</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/04_how-to-use-gpu-node-for-slurm-copy/</link>
      <pubDate>Fri, 11 Mar 2022 14:14:35 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/04_how-to-use-gpu-node-for-slurm-copy/</guid>
      <description>4. GPU node에서 tensorflow 코드 실행하기 2번 문서를 먼저 숙지하시기 바랍니다. 이 문서는 2번 문서의 Step 1, 2, 3 이후의 내용만을 다룹니다.
gpu-compute node에서는 Python만 사용 가능합니다.
Step 4. Setting up a conda environment conda environment를 새로 만들면서 cudatoolkit, tensorflow, torch를 설치합니다.
주의:
  각 사용자의 conda environment에 tensorflow, torch 뿐 아니라 cudatoolkit도 따로 설치됩니다.
  gpu-compute node와 cpu-compute node는 서로 다른 컴퓨터이므로 한쪽에서 만든 conda environment는 다른 쪽에서 사용할 수 없습니다.</description>
    </item>
    
    <item>
      <title>1. Introduction</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/01_intro/</link>
      <pubDate>Wed, 02 Mar 2022 22:24:04 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/01_intro/</guid>
      <description>요약 Chili Pepper는
 총 세 개의 컴퓨터로 구성된 컴퓨팅 클러스터입니다. CPU node 하나, GPU node 하나, 그리고 이 둘을 관리하는 proxy node로 구성되어 있습니다. Slurm이라는 job scheduler로 여러 user의 작업을 각 node에 효율적으로 할당합니다. 각 user는 자신만의 conda environment를 스스로 생성하여 사용합니다. 기본적으로 non-interactive입니다. User가 자신의 로컬 컴퓨터에서 작성 및 테스트한 코드를 서버에 제출하면 서버가 해당 코드를 실행합니다. 주피터 노트북 환경은 실행 가능하지만, 꼭 필요한 경우에 한하여 요청 시 제공합니다.</description>
    </item>
    
    <item>
      <title>SLURM Documentation</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/</link>
      <pubDate>Mon, 28 Feb 2022 22:39:38 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/</guid>
      <description>1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.
If you need more information, Please Visit https://slurm.schedmd.com/overview.html
2. Basic SLURM Command   sbatch
 Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output &amp;gt; Submitted batch job 210 # job_id: 210     squeue</description>
    </item>
    
    <item>
      <title>How do I use the Chili Pepper Cluster?</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/using-slurm-copy/</link>
      <pubDate>Mon, 21 Feb 2022 12:12:53 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/using-slurm-copy/</guid>
      <description>Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q&amp;amp;A channel in the Slack Group.
Step 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/.</description>
    </item>
    
    <item>
      <title>Creating Your Custom Jupyter Kernel from a Virtual Environment</title>
      <link>https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/</link>
      <pubDate>Sun, 28 Nov 2021 22:24:04 +0900</pubDate>
      
      <guid>https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/</guid>
      <description>Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.
1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way.</description>
    </item>
    
  </channel>
</rss>
