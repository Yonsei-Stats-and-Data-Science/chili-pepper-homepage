[{"content":"1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.\nIf you need more information, Please Visit https://slurm.schedmd.com/overview.html\n2. Basic SLURM Command   sbatch\n Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output \u0026gt; Submitted batch job 210 # job_id: 210     squeue\n View the queue  1 2 3 4 5  $ squeue # output \u0026gt; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 210 all test1 ghk R 0:29 1 cpu-compute     scancel\n Cancel a SLURM job  1  $ scancel [YOUR_JOBID]     sinfo\n See the state of system  1 2 3 4 5  $ sinfo # output \u0026gt; PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up infinite 2 idle cpu-compute,gpu-compute     smap\n graphically view information about SLURM job    For more information about SLURM command, please visit website below.\n  SLURM Workload Manager\n  SLURM Command Cheatsheet\n  SLURM Reference Sheet(NeSI)\n  Submit your first job(NeSI)\n  3. How to make SLURM batch script? \u0026lsquo;Job submission file\u0026rsquo; is the official SLURM name for the file you use to submit your program and ask for resources from the job scheduler. In this document, we will be using it ‘batch script’ or ‘script’.\nBasic example 1 2 3 4 5 6 7  #!/bin/bash #  # SBATCH --job-name=basic # SBATCH --mem=1gb # SBATCH --ntasks=1 # SBATCH --time=01:00 # SBATCH --output=/mnt/nas/users/testuser/basic.log   Asking 1 tasks, running for no more than 1 minutes limit memory less than 1gb. If any problem with your job, log file(in this case, \u0026lsquo;basic.log\u0026rsquo;) have information to help troubleshoot the issue.\nYou can also use gpu-nodes by using \u0026lsquo;—gres\u0026rsquo; option. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11  #!/bin/bash #  #SBATCH --job-name=basic #SBATCH --nodes=1 #SBATCH --gres=gpu:2 # max : 2 #SBATCH --time=01:00 #SBATCH --account=testuser #SBATCH --partition=all #SBATCH --output=/mnt/nas/users/testuser/torch.log source activate /opt/miniconda/envs/pytorch \u0026amp;\u0026amp; python /mnt/nas/users/testuser/main.py   1 2 3 4 5 6 7 8 9 10  # main.py import torch print(\u0026#39;is cuda avaiable? \u0026#39;, torch.cuda.is_available()) print(\u0026#39;how many cuda devices? \u0026#39;,torch.cuda.device_count()) print(\u0026#39;get first cuda device name =\u0026gt; \u0026#39;, torch.cuda.get_device_name(0)) print(\u0026#39;*** MEMORY INFO ***\u0026#39;) t = torch.cuda.get_device_properties(0).total_memory print(\u0026#39;total memory =\u0026gt; \u0026#39;, t) print(\u0026#39;total memory: \u0026#39;, t)   The job can then be submitted through sbatch\n1 2 3 4 5 6 7  $ sbatch basic.sh $ cat torch.log \u0026gt; is cuda avaiable? True \u0026gt; how many cuda devices? 2 \u0026gt; get first cuda device name =\u0026gt; Tesla T4 \u0026gt; ***MEMORY INFO*** \u0026gt; total memory =\u0026gt; 16879583232   Beacuse we only have 2 gpu machine, this option can’t be set more than 2\nFor the convenience of users, we provide SLURM job configurator page in our website. Please visit SLURM job configurator and make your own SLURM batch script!\n","permalink":"https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/","summary":"1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.\nIf you need more information, Please Visit https://slurm.schedmd.com/overview.html\n2. Basic SLURM Command   sbatch\n Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output \u0026gt; Submitted batch job 210 # job_id: 210     squeue","title":"SLURM Documentation"},{"content":"Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q\u0026amp;A channel in the Slack Group.\nStep 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/. For example, the home directory for the user dummyuser will be /mnt/nas/users/dummyuser/. The home directory is the recommended directory for users to store scripts, data and configuration files for using the Chili Pepper cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # with the tree command the home directory will typically look like this /mnt/nas/users/dummyuser/ ├── .bash_history ├── .bash_logout ├── .bashrc ├── .conda ├── .config ├── GettingStarted.md ├── .gnupg ├── .ipynb_checkpoints ├── .ipython ├── .jupyter ├── .local ├── logs ├── .npm ├── .profile ├── .python_history ├── some_script.sh ├── .ssh └── .viminfo   Step 2 - Data Transfer With cluster access information(SSH) provided by the administrator, you can send and recieve files from and to the cluster with scp. The dummyuser can send various local files to the cluster in the following fashion. Note that for security purposes the default port for SSH is not 22. The administrator will inform you of this information upon providing access information to the cluster.\nSending a local file(some_file.txt) to the remote home directory 1  scp some_file.txt dummyuser@hpc.stat.yonsei.ac.kr:~/   Sending a local directory(some_files/) to the remote home directory 1 2  scp -r some_files dummyuser@hpc.stat.yonsei.ac.kr:~/ # /mnt/nas/users/dummyuser/some_files/ will be created   Recieving a remote file(some_file.txt) in the home directory to the current local directory 1  scp dummyuser@hpc.stat.yonsei.ac.kr:~/some_file.txt   Recieving a remote directory(/mnt/nas/users/dummyuser/some_files) in the home directory to the current local directory 1  scp -r dummyuser@hpc.stat.yonsei.ac.kr:~/some_files ./   Other various options for scp exist. More information on this topic can be found in this article. Also users can use GitHub or GitLab to upload and download source code to the cluster. This will be handled in a seperate article.\nStep 3 - Writing a SBATCH script for SLURM. From the homepage the SLURM batch scripting tool is available. Let\u0026rsquo;s look at the sample script(/mnt/nas/users/dummyuser/test_script.sh) created by using the tool. The first-half(line 1 ~ 11) of the script consists of directives and parameters for the slurm job. Each user can set the number of nodes, the time for the job to occupy the number of nodes, the location for the output logfile. There are more options available for submitting a job. Additional resources for SBATCH arguments can be found here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #!/bin/bash # The interpreter used to execute the script #“#SBATCH” directives that convey submission options: #SBATCH --job-name=conda-env-create #SBATCH --nodes=1 #SBATCH --time=15:00 #SBATCH --account=dummyuser #SBATCH --partition=all #SBATCH --output=/mnt/nas/users/dummyuser/conda.log # The application(s) to execute along with its input arguments and options: CONDA_BIN_PATH=/opt/miniconda/bin ENV_NAME=myenv ENV_PATH=/mnt/nas/users/$(whoami)/.conda/envs/$ENV_NAME $CONDA_BIN_PATH/conda env remove --prefix $ENV_PATH $CONDA_BIN_PATH/conda create -y --prefix $ENV_PATH python=3.8 pandas numpy scikit-learn source $CONDA_BIN_PATH/activate $ENV_PATH \u0026amp;\u0026amp; pip freeze   From line 15 to the end of the script are actual bash commands for the node to execute.\n Line 15~16 creates two local variables(ENV_NAME and ENV_PATH). In the above script a conda environment named myenv will be created under /mnt/nas/users/dummyuser/.conda/envs/myenv. Line 18 will remove the environment in ENV_PATH if it is present. Line 19 will create a conda environment in ENV_PATH thanks to the -y(--yes) flag. This environment will have a Python interpreter of version 3.8 along with listed packages(pandas, numpy and scikit-learn). Line 20 will activate the conda environment in ENV_PATH and then sequentially run a pip freeze to the stdout. Note that the stdout is saved in the log file from line 11(/mnt/nas/users/dummyuser/conda.log).  To actually run a data science job, the only thing you have to do is to change the required packages for your environment, modify the pip freeze into python your_script_to_run.py.\nStep 4 - Submitting the script The submission of the script from the above is very simple.\n1  sbatch test_script.sh   You can check the current job queue with the following command.\n1  squeue   When you want to cancel the job you have submitted, get the JOBID from the squeue command and use the scancel command in the following fashion. Suppose the JOBID is 23.\n1  scancel 23   Note that ordinary users cannot cancel jobs that belong to other users, but the administrator can.\n","permalink":"https://hpc.stat.yonsei.ac.kr/docs/using-slurm/","summary":"Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q\u0026amp;A channel in the Slack Group.\nStep 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/.","title":"How do I use the Chili Pepper Cluster?"},{"content":"Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.\n1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way. Note that specifying ipykernel package will make the user easily create jupyter kernels from that environment. Also users can create virtual environments with various python versions(3.6 ~ 3.9). Activate the environment. If any warnings occur, read the warning and do the recommended procedure. Most of the time you will need to refresh your shell with bash.  1 2 3 4 5 6 7 8  # create environemnt conda create -n NAME_OF_VIRTUAL_ENV python=3.8 ipykernel # refresh your shell bash # activate environemnt conda activate NAME_OF_VIRTUAL_ENV   Install packages via pip.  1 2 3 4 5  # directly pip install PACKAGE_NAME # requirements.txt pip install -r YOUR_REQUIREMENTS.txt   Add the virtual environment as a kernel. This will be only available to each user.  1  python -m ipykernel install --user --name NAME_OF_VIRTUAL_ENV --display-name \u0026#34;[displayKenrelName]\u0026#34;   R  Launch a bash session via JupyterHub or SSH.  1  bash   Users then can create a virtual environment in the following way.  1 2 3 4 5 6 7 8  # create environemnt conda create -n NAME_OF_VIRTUAL_ENV r-essentials r-base r-irkernel # refresh your shell bash # activate environemnt conda activate NAME_OF_VIRTUAL_ENV   Install packages via install.packages().  1 2  # directly Rscript -e \u0026#39;install.packages(c(\u0026#34;dplyr\u0026#34;))\u0026#39;   Add the virtual environment as a kernel. This will be only available to each user.  1  Rscript -e \u0026#34;IRkernel::installspec(name=\u0026#39;myRkernel\u0026#39;, displayname=\u0026#39;My R Kernel\u0026#39;)\u0026#34;   Removing Kernels To remove kernels use the jupyter command in terminal.\nView your current kernel list with the following command from a bash terminal.\n1  jupyter kernelspec list   Remove kernel with the following command.\n1  jupyter kernelspec remove KERNELNAME   ","permalink":"https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/","summary":"Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.\n1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way.","title":"Creating Your Custom Jupyter Kernel from a Virtual Environment"}]