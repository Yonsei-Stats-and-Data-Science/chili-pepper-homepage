[{"content":"How to use GPU node for SLURM Step 1. Export your conda setting conda 환경을 동일하게 맞춰주기 위해 local에서 생성된 가상환경으로부터 환경설정 파일을 만들고, 서버에서 conda 가상환경을 만들 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11  # export conda setting conda activate [YOUR ENV NAME] conda env export -n [ENV NAME] -f [FILENAME].yml --no-builds # 이러면 조금은 해결되긴 함 # create environment from file conda create --name [YOUR ENV NAME] python = [VESTION] # same env name in yml file conda env create -f [FILENAME].yml conda env create -p [prefix path] -f [filename].yml   —no-builds 옵션은 서로 다른 OS에서 conda 가상환경 파일 내 패키지들의 버전 충돌을 방지하기 위한 것이다. 다만 경우에 따라 완전히 문제를 예방하지는 못하기 때문에 사용자들이 직접 env 파일을 수정하는 상황이 발생할 수 있다. 이런 경우 ResolvePackageNotFound 아래의 패키지들을 env 파일에서 삭제해주면 문제가 해결된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  conda env create -f test.yml Collecting package metadata: done Solving environment: failed # yml 파일에서 아래에 등장하는 패키지들을 지워주면 된다. ResolvePackageNotFound: - libgfortran==3.0.1=h93005f0_2 # --no-builds는 패키지 버전 옆의 빌드 정보를 제거한다. - pyzmq==17.0.0=py36h1de35cc_1 - python==3.6.6=h4a56312_1003 - prompt_toolkit==1.0.15=py36haeda067_0 - libiconv==1.15=h1de35cc_1004 - sqlite==3.25.3=ha441bb4_0 - six==1.11.0=py36h0e22d5e_1 - cryptography==2.3.1=py36hdbc3d79_1000 - openssl==1.0.2p=h1de35cc_1002 - libxml2==2.9.8=hf14e9c8_1005 - libcxxabi==4.0.1=hebd6815_0 - matplotlib==2.2.3=py36h0e0179f_0 - ptyprocess==0.5.2=py36he6521c3_0   Step 2. make env file and upload file to user’s workspace 로컬에서 만들어진 env 파일을 서버로 옮길 때는 scp를 사용한다. 간단한 사용법은 아래와 같다.\n1 2 3 4 5 6 7  # scp 사용법 # File upload scp [FILEPATH] ghk@[IP address]:~/ # upload from local, ~/ means home directory # File download scp ghk@[IP address]:[FILEPATH] ./ # scp [server file path] [local save path] # directory 전체 다운, 업로드 할 때는 -r 옵션을 사용한다.   Windows 사용자라면 WinSCP 라는 이름의 프로그램을 사용하면 로컬과 서버 간 파일 전송을 보다 쉽게 처리할 수 있다.\n 💡 env file을 이용한 conda 환경 설정은 로컬과 서버 작업 환경을 동일하게 설정할 수 있는 신뢰할 수 있는 방법이다. 그러나 conda 환경 설정 과정이 너무 번거롭다면 requirements.txt를 만들어 패키지 버전만 관리해도 충돌을 방지할 수 있다.  1 2 3 4 5 6 7  conda install --force-reinstall -y -q -c conda-forge --file requirements.txt # --force-reinstall : Install the package even if it already exists. # -y : Yes, do not ask for confirmation. # -q : Quiet, do not display progress bar. # -c : Channels, additional channels to search for packages # conda-forge is recommended   Step 3. make SLRUM batch script and run code in server 앞선 단계에서 conda환경이 잘 만들어졌다면 해당 가상환경을 activate하여 코드를 돌리는 SLURM batch script를 작성할 수 있다. 사용자들의 이해를 돕기 위해 TensorFlow 공식 페이지에 게시된 초보자용 튜토리얼 코드를 SLURM을 통해 실행시키는 예제를 공유한다. 먼저, 튜토리얼 코드는 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # tensor.py import tensorflow as tf mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(x_train, y_train, epochs=5) model.evaluate(x_test, y_test, verbose=2)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  #!/bin/bash # #SBATCH --job-name=gpu-tensor-test #SBATCH --mem=4gb #SBATCH --nodelist=gpu-compute #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --gres=gpu:1 #SBATCH --time=00:20:00 #SBATCH --account=ghk #SBATCH --partition=all #SBATCH --output=/mnt/nas/users/ghk/code/gputest.log CONDA_BIN_PATH=/opt/miniconda/bin ENV_NAME=tensor ENV_PATH=/mnt/nas/users/$(whoami)/.conda/envs/$ENV_NAME ## $CONDA_BIN_PATH/conda env remove --prefix $ENV_PATH ## $CONDA_BIN_PATH/conda create -y --prefix $ENV_PATH python=3.8 tensorflow pandas numpy source $CONDA_BIN_PATH/activate $ENV_PATH ## pip uninstall keras ## pip install keras==2.6.0 python /mnt/nas/users/ghk/code/tensor.py    —job-name: 수행할 작업의 이름 —mem: memory limit —nodelist: 작업할 노드의 이름 —ntasks: 작업의 수 —cpus-per-task: 각 작업에서 사용할 cpu 코어의 수 —gres=gpu : 작업에서 사용할 gpu의 개수, gpu-compute 노드에는 총 2개의 gpu가 사용 가능하다. —time: 작업 제한시간 —account: 해당 작업을 수행하는 계정의 이름 —partition: group of nodes with specific characteristics —output: 코드 실행 결과 log  제시한 대로 python 파일과 batch script 파일이 잘 만들어졌다면 sbatch 명령어를 입력하여 계산을 실행할 수 있다.\n1 2  sbatch tensor.sh smap -i 1 # 작업 현황을 1초마다 갱신하여 보여준다. ctrl+c로 escape 할 수 있다.   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ghk@proxy:~/code$ cat gputest.log 2022-03-15 14:27:34.877232: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-03-15 14:27:34.887611: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2022-03-15 14:27:38.063857: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2) Epoch 1/5 1875/1875 [==============================] - 36s 19ms/step - loss: 0.2993 - accuracy: 0.9140 Epoch 2/5 1875/1875 [==============================] - 18s 10ms/step - loss: 0.1436 - accuracy: 0.9575 Epoch 3/5 1875/1875 [==============================] - 17s 9ms/step - loss: 0.1080 - accuracy: 0.9675 Epoch 4/5 1875/1875 [==============================] - 19s 10ms/step - loss: 0.0866 - accuracy: 0.9739 Epoch 5/5 1875/1875 [==============================] - 52s 28ms/step - loss: 0.0750 - accuracy: 0.9762 313/313 - 4s - loss: 0.0782 - accuracy: 0.9779 [0.078231580555439, 0.9779000282287598]   SLURM batch script를 사용자들이 보다 편하게 만들 수 있도록 SLURM Job Configurator 를 새롭게 작성하였다. 사용자들은 gpu 옵션을 체크하거나 해제하여 gpu-compute node 사용 여부를 결정할 수 있다. 해당되는 옵션을 체크하고 Print 버튼을 누르면 손쉽게 batch script를 작성할 수 있다.\n더 알아보기 Submitting a slurm job script\nSLRUM Job Examples\nTensorFlow on the HPC Clusters\n","permalink":"https://hpc.stat.yonsei.ac.kr/docs/how-to-use-gpu-node-for-slurm/","summary":"How to use GPU node for SLURM Step 1. Export your conda setting conda 환경을 동일하게 맞춰주기 위해 local에서 생성된 가상환경으로부터 환경설정 파일을 만들고, 서버에서 conda 가상환경을 만들 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11  # export conda setting conda activate [YOUR ENV NAME] conda env export -n [ENV NAME] -f [FILENAME].yml --no-builds # 이러면 조금은 해결되긴 함 # create environment from file conda create --name [YOUR ENV NAME] python = [VESTION] # same env name in yml file conda env create -f [FILENAME].","title":"How to Use GPU Node for SLURM"},{"content":"요약 Chilli Pepper는\n 총 세 개의 컴퓨터로 구성된 컴퓨팅 클러스터입니다. Cpu node 하나와 gpu node 하나, 그리고 이 둘을 관리하는 proxy node로 구성되어 있습니다. Slurm이라는 job scheduler로 여러 사용자의 작업을 각 node에 효율적으로 할당합니다. 각 사용자는 자신만의 conda environment를 스스로 생성하여 사용합니다. 기본적으로 non-interactive입니다. 사용자가 자신의 로컬 컴퓨터에서 작성 및 시험한 코드를 서버에 제출하면 서버가 해당 코드를 실행합니다. 주피터 노트북 환경은 실행 가능하지만, 꼭 필요한 경우에 한하여 요청 시 제공합니다. 이는 노트북 환경은 작업 단위가 아닌 시간 단위로 실행되기 때문에 서버의 RAM 자원을 지나치게 많이 점유하고 다른 작업의 실행을 방해하기 때문입니다.  1. 클러스터 구성 1. cpu node  Name: cpu-compute CPU: Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz 32 cores RAM: 128GB OS: ubuntu 18.04.6 LTS conda version: 4.11.0  2. gpu-compute node  Name: gpu-compute CPU: Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz 16 cores GPU: NVIDIA® Tesla® T4 16GB x 2 RAM: 80GB OS: ubuntu 18.04.6 LTS conda version: 4.6.14 GPU driver version: 418.67  3. proxy node  Name: proxy CPU: Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz 2cores RAM: 4GB OS: ubuntu 18.04.6 LTS  특이사항   Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz가 18코어 제품임에도 불구하고 세 컴퓨터에 각각 32코어, 16코어, 2코어로 장착되어 있는 것은 이 세 컴퓨터가 네이버 클라우드 플랫폼에서 호스팅되고 있기 때문입니다. 네이버 클라우드 플랫폼에서 각 컴퓨터에 정해진 코어 수를 할당합니다.\n  proxy node는 사용자가 접속하여 job을 제출하기 위한 용도로만 쓰이는 컴퓨터이므로 성능이 낮습니다.\n  사용자는 proxy에만 접속할 수 있으며, cpu-compute와 gpu-copute에는 접속할 수 없습니다.\n  2. Job scheduler 3. Conda environment 4. ","permalink":"https://hpc.stat.yonsei.ac.kr/docs/overview/","summary":"요약 Chilli Pepper는\n 총 세 개의 컴퓨터로 구성된 컴퓨팅 클러스터입니다. Cpu node 하나와 gpu node 하나, 그리고 이 둘을 관리하는 proxy node로 구성되어 있습니다. Slurm이라는 job scheduler로 여러 사용자의 작업을 각 node에 효율적으로 할당합니다. 각 사용자는 자신만의 conda environment를 스스로 생성하여 사용합니다. 기본적으로 non-interactive입니다. 사용자가 자신의 로컬 컴퓨터에서 작성 및 시험한 코드를 서버에 제출하면 서버가 해당 코드를 실행합니다. 주피터 노트북 환경은 실행 가능하지만, 꼭 필요한 경우에 한하여 요청 시 제공합니다.","title":"Overview"},{"content":"1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.\nIf you need more information, Please Visit https://slurm.schedmd.com/overview.html\n2. Basic SLURM Command   sbatch\n Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output \u0026gt; Submitted batch job 210 # job_id: 210     squeue\n View the queue  1 2 3 4 5  $ squeue # output \u0026gt; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 210 all test1 ghk R 0:29 1 cpu-compute     scancel\n Cancel a SLURM job  1  $ scancel [YOUR_JOBID]     sinfo\n See the state of system  1 2 3 4 5  $ sinfo # output \u0026gt; PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up infinite 2 idle cpu-compute,gpu-compute     smap\n graphically view information about SLURM job    For more information about SLURM command, please visit website below.\n  SLURM Workload Manager\n  SLURM Command Cheatsheet\n  SLURM Reference Sheet(NeSI)\n  Submit your first job(NeSI)\n  3. How to make SLURM batch script? \u0026lsquo;Job submission file\u0026rsquo; is the official SLURM name for the file you use to submit your program and ask for resources from the job scheduler. In this document, we will be using it ‘batch script’ or ‘script’.\nBasic example 1 2 3 4 5 6 7  #!/bin/bash #  # SBATCH --job-name=basic # SBATCH --mem=1gb # SBATCH --ntasks=1 # SBATCH --time=01:00 # SBATCH --output=/mnt/nas/users/testuser/basic.log   Asking 1 tasks, running for no more than 1 minutes limit memory less than 1gb. If any problem with your job, log file(in this case, \u0026lsquo;basic.log\u0026rsquo;) have information to help troubleshoot the issue.\nYou can also use gpu-nodes by using \u0026lsquo;—gres\u0026rsquo; option. Here is an example.\n1 2 3 4 5 6 7 8 9 10 11  #!/bin/bash #  #SBATCH --job-name=basic #SBATCH --nodes=1 #SBATCH --gres=gpu:2 # max : 2 #SBATCH --time=01:00 #SBATCH --account=testuser #SBATCH --partition=all #SBATCH --output=/mnt/nas/users/testuser/torch.log source activate /opt/miniconda/envs/pytorch \u0026amp;\u0026amp; python /mnt/nas/users/testuser/main.py   1 2 3 4 5 6 7 8 9 10  # main.py import torch print(\u0026#39;is cuda avaiable? \u0026#39;, torch.cuda.is_available()) print(\u0026#39;how many cuda devices? \u0026#39;,torch.cuda.device_count()) print(\u0026#39;get first cuda device name =\u0026gt; \u0026#39;, torch.cuda.get_device_name(0)) print(\u0026#39;*** MEMORY INFO ***\u0026#39;) t = torch.cuda.get_device_properties(0).total_memory print(\u0026#39;total memory =\u0026gt; \u0026#39;, t) print(\u0026#39;total memory: \u0026#39;, t)   The job can then be submitted through sbatch\n1 2 3 4 5 6 7  $ sbatch basic.sh $ cat torch.log \u0026gt; is cuda avaiable? True \u0026gt; how many cuda devices? 2 \u0026gt; get first cuda device name =\u0026gt; Tesla T4 \u0026gt; ***MEMORY INFO*** \u0026gt; total memory =\u0026gt; 16879583232   Beacuse we only have 2 gpu machine, this option can’t be set more than 2\nFor the convenience of users, we provide SLURM job configurator page in our website. Please visit SLURM job configurator and make your own SLURM batch script!\n","permalink":"https://hpc.stat.yonsei.ac.kr/docs/slurm-documentation/","summary":"1. What is SLURM? Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained.\nIf you need more information, Please Visit https://slurm.schedmd.com/overview.html\n2. Basic SLURM Command   sbatch\n Submit a batch script to SLURM  1 2 3 4  $ sbatch [YOUR_SCRIPT] # output \u0026gt; Submitted batch job 210 # job_id: 210     squeue","title":"SLURM Documentation"},{"content":"Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q\u0026amp;A channel in the Slack Group.\nStep 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/. For example, the home directory for the user dummyuser will be /mnt/nas/users/dummyuser/. The home directory is the recommended directory for users to store scripts, data and configuration files for using the Chili Pepper cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # with the tree command the home directory will typically look like this /mnt/nas/users/dummyuser/ ├── .bash_history ├── .bash_logout ├── .bashrc ├── .conda ├── .config ├── GettingStarted.md ├── .gnupg ├── .ipynb_checkpoints ├── .ipython ├── .jupyter ├── .local ├── logs ├── .npm ├── .profile ├── .python_history ├── some_script.sh ├── .ssh └── .viminfo   Step 2 - Data Transfer With cluster access information(SSH) provided by the administrator, you can send and recieve files from and to the cluster with scp. The dummyuser can send various local files to the cluster in the following fashion. Note that for security purposes the default port for SSH is not 22. The administrator will inform you of this information upon providing access information to the cluster.\nSending a local file(some_file.txt) to the remote home directory 1  scp some_file.txt dummyuser@hpc.stat.yonsei.ac.kr:~/   Sending a local directory(some_files/) to the remote home directory 1 2  scp -r some_files dummyuser@hpc.stat.yonsei.ac.kr:~/ # /mnt/nas/users/dummyuser/some_files/ will be created   Recieving a remote file(some_file.txt) in the home directory to the current local directory 1  scp dummyuser@hpc.stat.yonsei.ac.kr:~/some_file.txt   Recieving a remote directory(/mnt/nas/users/dummyuser/some_files) in the home directory to the current local directory 1  scp -r dummyuser@hpc.stat.yonsei.ac.kr:~/some_files ./   Other various options for scp exist. More information on this topic can be found in this article. Also users can use GitHub or GitLab to upload and download source code to the cluster. This will be handled in a seperate article.\nStep 3 - Writing a SBATCH script for SLURM. From the homepage the SLURM batch scripting tool is available. Let\u0026rsquo;s look at the sample script(/mnt/nas/users/dummyuser/test_script.sh) created by using the tool. The first-half(line 1 ~ 11) of the script consists of directives and parameters for the slurm job. Each user can set the number of nodes, the time for the job to occupy the number of nodes, the location for the output logfile. There are more options available for submitting a job. Additional resources for SBATCH arguments can be found here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #!/bin/bash # The interpreter used to execute the script #“#SBATCH” directives that convey submission options: #SBATCH --job-name=conda-env-create #SBATCH --nodes=1 #SBATCH --time=15:00 #SBATCH --account=dummyuser #SBATCH --partition=all #SBATCH --output=/mnt/nas/users/dummyuser/conda.log # The application(s) to execute along with its input arguments and options: CONDA_BIN_PATH=/opt/miniconda/bin ENV_NAME=myenv ENV_PATH=/mnt/nas/users/$(whoami)/.conda/envs/$ENV_NAME $CONDA_BIN_PATH/conda env remove --prefix $ENV_PATH $CONDA_BIN_PATH/conda create -y --prefix $ENV_PATH python=3.8 pandas numpy scikit-learn source $CONDA_BIN_PATH/activate $ENV_PATH \u0026amp;\u0026amp; pip freeze   From line 15 to the end of the script are actual bash commands for the node to execute.\n Line 15~16 creates two local variables(ENV_NAME and ENV_PATH). In the above script a conda environment named myenv will be created under /mnt/nas/users/dummyuser/.conda/envs/myenv. Line 18 will remove the environment in ENV_PATH if it is present. Line 19 will create a conda environment in ENV_PATH thanks to the -y(--yes) flag. This environment will have a Python interpreter of version 3.8 along with listed packages(pandas, numpy and scikit-learn). Line 20 will activate the conda environment in ENV_PATH and then sequentially run a pip freeze to the stdout. Note that the stdout is saved in the log file from line 11(/mnt/nas/users/dummyuser/conda.log).  To actually run a data science job, the only thing you have to do is to change the required packages for your environment, modify the pip freeze into python your_script_to_run.py.\nStep 4 - Submitting the script The submission of the script from the above is very simple.\n1  sbatch test_script.sh   You can check the current job queue with the following command.\n1  squeue   When you want to cancel the job you have submitted, get the JOBID from the squeue command and use the scancel command in the following fashion. Suppose the JOBID is 23.\n1  scancel 23   Note that ordinary users cannot cancel jobs that belong to other users, but the administrator can.\n","permalink":"https://hpc.stat.yonsei.ac.kr/docs/using-slurm/","summary":"Intro This documentation will go over the basics of using the Chili Pepper cluster. Please go through this documentation step-by-step. Contact the server administrator via email or use the Q\u0026amp;A channel in the Slack Group.\nStep 1 - Understanding the Cluster Structure Chili Pepper cluster has a NAS which contains user home directories and other shared files. All users and groups are consistent across all nodes. The prefix for the user directory is /mnt/nas/users/.","title":"How do I use the Chili Pepper Cluster?"},{"content":"Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.\n1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way. Note that specifying ipykernel package will make the user easily create jupyter kernels from that environment. Also users can create virtual environments with various python versions(3.6 ~ 3.9). Activate the environment. If any warnings occur, read the warning and do the recommended procedure. Most of the time you will need to refresh your shell with bash.  1 2 3 4 5 6 7 8  # create environemnt conda create -n NAME_OF_VIRTUAL_ENV python=3.8 ipykernel # refresh your shell bash # activate environemnt conda activate NAME_OF_VIRTUAL_ENV   Install packages via pip.  1 2 3 4 5  # directly pip install PACKAGE_NAME # requirements.txt pip install -r YOUR_REQUIREMENTS.txt   Add the virtual environment as a kernel. This will be only available to each user.  1  python -m ipykernel install --user --name NAME_OF_VIRTUAL_ENV --display-name \u0026#34;[displayKenrelName]\u0026#34;   R  Launch a bash session via JupyterHub or SSH.  1  bash   Users then can create a virtual environment in the following way.  1 2 3 4 5 6 7 8  # create environemnt conda create -n NAME_OF_VIRTUAL_ENV r-essentials r-base r-irkernel # refresh your shell bash # activate environemnt conda activate NAME_OF_VIRTUAL_ENV   Install packages via install.packages().  1 2  # directly Rscript -e \u0026#39;install.packages(c(\u0026#34;dplyr\u0026#34;))\u0026#39;   Add the virtual environment as a kernel. This will be only available to each user.  1  Rscript -e \u0026#34;IRkernel::installspec(name=\u0026#39;myRkernel\u0026#39;, displayname=\u0026#39;My R Kernel\u0026#39;)\u0026#34;   Removing Kernels To remove kernels use the jupyter command in terminal.\nView your current kernel list with the following command from a bash terminal.\n1  jupyter kernelspec list   Remove kernel with the following command.\n1  jupyter kernelspec remove KERNELNAME   ","permalink":"https://hpc.stat.yonsei.ac.kr/docs/configuring-jupyter-kernel/","summary":"Creating a Python Kernel Admin Prerequisite Admin must enable conda for all users by creating a symlink. This will let users create their own virtual environment by using the conda create command.\n1  ln -s /opt/conda/bin/conda /usr/local/bin/conda   User Instructions Python  Users can launch a terminal session with either JupyterHub or SSH connections. Launch a bash session.  1  bash   Users then can create a virtual environment in the following way.","title":"Creating Your Custom Jupyter Kernel from a Virtual Environment"}]